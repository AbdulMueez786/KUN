# -*- coding: utf-8 -*-
"""KunFYP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oslZEmcVdI9JHqmC0lmdgosti1xrEGiQ
"""

#References
# https://www.thepythoncode.com/article/using-speech-recognition-to-convert-speech-to-text-python
# https://towardsdatascience.com/audio-deep-learning-made-simple-automatic-speech-recognition-asr-how-it-works-716cfce4c706

"""# **Get Text from Audio**"""

from google.colab import drive
drive.mount('/content/drive')

pip install SpeechRecognition moviepy

pip install Pydub pydub

import speech_recognition as sr
import moviepy.editor as mp

clip = mp.VideoFileClip(r"/content/drive/MyDrive/KunFYP/video 14.mp4")
clip.audio.write_audiofile(r"/content/drive/MyDrive/KunFYP/audio14.wav")

# importing libraries 
import speech_recognition as sr 
import csv
import os 
from pydub import AudioSegment
from pydub.silence import split_on_silence
from pydub.utils import make_chunks

# create a speech recognition object
r = sr.Recognizer()

textArr = []

# a function that splits the audio file into chunks
# and applies speech recognition
def get_large_audio_transcription(path):
    """
    Splitting the large audio file into chunks
    and apply speech recognition on each of these chunks
    """
    # open the audio file using pydub
    sound = AudioSegment.from_wav(path)  
    # split audio sound where silence is 700 miliseconds or more and get chunks
    audio_size = 10 * 1000
    chunks = make_chunks(sound, audio_size)
    #chunks = split_on_silence(sound,
        # experiment with this value for your target audio file
    #    min_silence_len = 700,
    #    # adjust this per requirement
    #    silence_thresh = sound.dBFS-5,
    #    # keep the silence for 1 second, adjustable as well
    #    keep_silence=700,
    #)
    #folder_name = "/content/drive/MyDrive/KunFYP/audio_dataset"
    folder_name = "/content/drive/MyDrive/KunFYP/audio_dataset2"
    # create a directory to store the audio chunks
    if not os.path.isdir(folder_name):
        os.mkdir(folder_name)

    #get index to start from
    person = 'A' #A=Armughan, B = Abdul Mueez, C = Saad
    numbering_start = 0
    
    #numbering_path = "/content/drive/MyDrive/KunFYP/audio_numbering.csv"
    numbering_path = "/content/drive/MyDrive/KunFYP/audio_numbering2.csv"
    with open(numbering_path) as csv_file:
      csv_reader = csv.reader(csv_file)
      for row in csv_reader:
        if row[0] == person:
          numbering_start = int(row[1])
          break
    
    print(path)


    # process each chunk 
    for i, audio_chunk in enumerate(chunks, start=1):
        # export audio chunk and save it in
        # the `folder_name` directory.
        chunk_filename = os.path.join(folder_name, person+f"{i+numbering_start}.wav")
        audio_chunk.export(chunk_filename, format="wav")
    #     # recognize the chunk
        with sr.AudioFile(chunk_filename) as source:
            audio_listened = r.record(source)
            # try converting it to text
            try:
                text = r.recognize_google(audio_listened, language = "ur-PK")
            except sr.UnknownValueError as e:
                print("Error at line "+str(i+numbering_start))
                txt = []
                txt.append(f"{i+numbering_start}.wav")
                txt.append("سمجھ نہیں آئی")
                textArr.append(txt)
            else:
                txt = []
                txt.append(f"{i+numbering_start}.wav")
                txt.append(text)
                textArr.append(txt)

    print("video conversion done at line "+f"{i+numbering_start}")

    #append to csv file
    #with open("/content/drive/MyDrive/KunFYP/text_dataset.csv",'a',newline='') as file:
    # with open("/content/drive/MyDrive/KunFYP/text_dataset2.csv",'a',newline='') as file:
    #   writer = csv.writer(file)
    #   for x in range(len(textArr)):
    #     writer.writerow([textArr[x][0],textArr[x][1]])

    # #make changes to numbering file
    # number_data = []
    # with open(numbering_path) as csv_file:
    #   csv_reader = csv.reader(csv_file)
    #   for row in csv_reader:
    #     dt = []
    #     dt.append(row[0])
    #     if row[0] == person:
    #       dt.append(int(row[1])+len(textArr))
    #     else:
    #       dt.append(int(row[1]))
    #     number_data.append(dt)

    # with open(numbering_path,'w',newline='') as file:
    #   writer = csv.writer(file)
    #   for x in range(len(number_data)):
    #     writer.writerow([number_data[x][0],number_data[x][1]])
    
     
    return

path = "/content/drive/MyDrive/KunFYP/audio2.wav"
get_large_audio_transcription(path)

"""#**Clean and Correct Dataset**

"""

import numpy as np
import pandas as pd
import csv
import nltk
nltk.download('punkt') # one time execution
import re

#read dataset from csv
path = "/content/drive/MyDrive/KunFYP/text_dataset2.csv"

dataset = []
with open(path) as csv_file:
  csv_reader = csv.reader(csv_file)
  for row in csv_reader:
    data = []
    s = row[0]
    s2 = row[1]
    data.append(s)
    data.append(s2)
    dataset.append(data)

#add alphabet
collector = 0
for x in range(len(dataset)):
  if dataset[x][0] == "1.wav":
    collector+= 1
  if collector == 1:
    dataset[x][0] = "A"+dataset[x][0]
  elif collector == 2:
    dataset[x][0] = "B"+dataset[x][0]
  elif collector == 3:
    dataset[x][0] = "C"+dataset[x][0]

dataset2 = []
#remove sentences not understood
for x in range(len(dataset)):
  if dataset[x][1] != "سمجھ نہیں آئی":
    dataset2.append(dataset[x])

#count number of words on sentence length
short = 0
medium = 0
longs = 0

for x in range(len(dataset2)):
  if len(dataset2[x][1]) >= 0 and len(dataset2[x][1]) < 50:
    short+=1
  elif len(dataset2[x][1]) >= 50 and len(dataset2[x][1]) < 100:
    medium+=1
  else:
    longs+=1

print(f"short sentences: {short}, medium sentences: {medium}, long sentences: {longs}")

#save to new csv file, only short sentences in dataset
with open("/content/drive/MyDrive/KunFYP/text_dataset_improved2.csv",'w',newline='') as file:
  writer = csv.writer(file)
  writer.writerow(["Filename", "Text"])
  for x in range(len(dataset2)):
    writer.writerow([dataset2[x][0],dataset2[x][1]])

"""# **Generate Spectrograms**"""

import librosa

# Load the audio file
AUDIO_FILE = '/content/drive/MyDrive/KunFYP/audio_dataset2/A20.wav'
samples, sample_rate = librosa.load(AUDIO_FILE, sr=None)

import librosa.display
import matplotlib.pyplot as plt

# x-axis has been converted to time using our sample rate. 
# matplotlib plt.plot(y), would output the same figure, but with sample 
# number on the x-axis instead of seconds
plt.figure(figsize=(14, 5))
librosa.display.waveplot(samples, sr=sample_rate)

from IPython.display import Audio
Audio(AUDIO_FILE)

print ('Example shape ', samples.shape, 'Sample rate ', sample_rate, 'Data type', type(samples))
print (samples[22400:22420])

sgram = librosa.stft(samples)
librosa.display.specshow(sgram)

# use the mel-scale instead of raw frequency
sgram_mag, _ = librosa.magphase(sgram)
mel_scale_sgram = librosa.feature.melspectrogram(S=sgram_mag, sr=sample_rate)
librosa.display.specshow(mel_scale_sgram)

import numpy as np

# use the decibel scale to get the final Mel Spectrogram
mel_sgram = librosa.amplitude_to_db(mel_scale_sgram, ref=np.min)
librosa.display.specshow(mel_sgram, sr=sample_rate, x_axis='time', y_axis='mel')
plt.colorbar(format='%+2.0f dB')

"""# **libraries**"""

#libraries

import tensorflow as tf
import keras.backend as K
from keras.models import Model
from keras.layers import Input
from keras.layers import TimeDistributed
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers import Bidirectional
from keras.layers import Lambda
from keras.layers import Dropout
from keras.regularizers import l2
from keras.initializers import random_normal
from keras.utils.conv_utils import conv_output_length
from keras.layers import GaussianNoise
from keras.callbacks import ModelCheckpoint,EarlyStopping

import numpy as np
from keras import backend as K
from keras.models import Model, Sequential
from keras.layers.recurrent import SimpleRNN
from keras.layers import Dense, Activation, Bidirectional, Reshape,Flatten, Lambda, Input,\
    Masking, Convolution1D, BatchNormalization, GRU, Conv1D, RepeatVector, Conv2D,LSTM
from keras.optimizers import SGD, Adam
from keras.layers import ZeroPadding1D, Convolution1D, ZeroPadding2D,Bidirectional,Flatten,CuDNNGRU,Convolution2D, MaxPooling2D,MaxPooling1D, GlobalMaxPooling2D
from keras.layers import TimeDistributed, Dropout
from keras.layers.merge import add  # , # concatenate BAD FOR COREML
from keras.utils.conv_utils import conv_output_length
from keras.activations import relu
import pandas as pd
import librosa
import librosa.display
import matplotlib.pyplot as plt
import pandas as pd
from pathlib import Path
import sklearn.preprocessing as i
from keras.utils.np_utils import to_categorical
from keras import models  
import random
import IPython.display as ipd

"""# **Model Creation**

**Labels**
"""

import pandas as pd
from pathlib import Path
labels=[]
Files=[]
text=pd.read_csv("/content/drive/MyDrive/KunFYP/text_dataset_improved2.csv")
count=0
for i in text.values:      
        path = Path('/content/drive/MyDrive/KunFYP/audio_dataset2/'+i[0])
        if (path.is_file()==True and count!=5):
            Files.append(i[0])  
            labels.append(i[1])
            count+=1
            print("i")
print(len(Files))
print(len(labels))

"""**Spectrograms**

"""

import librosa
import librosa.display
import matplotlib.pyplot as plt
import numpy as np
from pathlib import Path
import csv


# x-axis has been converted to time using our sample rate. 
# matplotlib plt.plot(y), would output the same figure, but with sample 
# number on the x-axis instead of seconds
# Load the audio file

Audio_File = '/content/drive/MyDrive/KunFYP/audio_dataset2/'
paths=[]
counter=0
waves=[]

f = open('/content/drive/MyDrive/KunFYP/samples.csv', 'w')
writer = csv.writer(f)

for i in Files:
        path = Path(Audio_File+i)
        #paths.append(Audio_File+i)
        if (path.is_file()):
           samples, sample_rate = librosa.load(Audio_File+i, sr=None)
        #fig=plt.figure(figsize=(14, 5))
        #librosa.display.waveplot(samples, sr=sample_rate)
        #sgram = librosa.stft(samples)
        #librosa.display.specshow(sgram)
        # use the mel-scale instead of raw frequency
        #sgram_mag, _ = librosa.magphase(sgram)
        #mel_scale_sgram = librosa.feature.melspectrogram(S=sgram_mag, sr=sample_rate)
        #librosa.display.specshow(mel_scale_sgram)    
        # use the decibel scale to get the final Mel Spectrogram
        #mel_sgram = librosa.amplitude_to_db(mel_scale_sgram, ref=np.min)
        #librosa.display.specshow(mel_sgram, sr=sample_rate, x_axis='time', y_axis='mel')
        #plt.colorbar(format='%+2.0f dB')
           samples = librosa.resample(samples, sample_rate, 1000)
           waves.append(samples)
           writer.writerow(samples)
           print("i");
           print(i)
           print(samples);
         
f.close()

import csv

# open the file in the write mode
#f = open('/content/drive/MyDrive/KunFYP/samples.csv', 'w')

# create the csv writer
writer = csv.writer(f)


from pathlib import Path
import librosa
waves=[]
Audio_File = '/content/drive/MyDrive/KunFYP/audio_dataset2/'
for i in Files:
        path = Path(Audio_File+i)
        if (path.is_file()):            
            samples, sample_rate = librosa.load(Audio_File+i, sr=None)
            samples = librosa.resample(samples, sample_rate, 1000)
            waves.append(samples)
            print(samples)
            #all_wave = np.array(all_wave).reshape(-1,8000,1)
            #writer.writerow(samples)
# write a row to the csv file


# close the file
f.close()

#import pandas as pd
#from pathlib import Path
#waves=[]
#test=pd.read_csv("/content/drive/MyDrive/KunFYP/samples.csv")

#for i in test.values:
#         waves.append(i)
   
#print(waves)

print(np.array(waves).shape)

#all_wave = np.array(waves).reshape(5,10000,1)
#print(np.array(waves).shape,y.shape)

import sklearn.preprocessing as i
import numpy as np
from keras.utils.np_utils import to_categorical
from sklearn.model_selection import train_test_split

label_enconder = i.LabelEncoder()
y = label_enconder.fit_transform(labels)

all_wave = np.array(waves).reshape(-1,10000,1)
print(y.shape)
print(all_wave.shape)

x_train, x_valid, y_train, y_valid = train_test_split(np.array(all_wave),np.array(y),test_size=0.2,random_state=777)

#print(np.array(all_wave))
#print("Data")
#print(y)
#print(waves) 

#print(len(all_wave))
#print(x_train)
#print(y)
print(y_train)

print(y_valid)
#print(len(x_train))
#print(np.array(all_wave).shape)

print(labels)
print(y)

!pip install git+https://www.github.com/keras-team/keras-contrib.git

print(labels,len(labels))

inputs = Input(shape=(10000,1))
x = BatchNormalization(axis=-1, momentum=0.99, epsilon=1e-3, center=True, scale=True)(inputs)
#First Conv1D layer
x = Conv1D(8,13, padding='valid', activation='relu', strides=1)(x)
x = MaxPooling1D(3)(x)
x = Dropout(0.3)(x)
#Second Conv1D layer
x = Conv1D(16, 11, padding='valid', activation='relu', strides=1)(x)
x = MaxPooling1D(3)(x)
x = Dropout(0.3)(x)
#Third Conv1D layer
x = Conv1D(32, 9, padding='valid', activation='relu', strides=1)(x)
x = MaxPooling1D(3)(x)
x = Dropout(0.3)(x)
x = BatchNormalization(axis=-1, momentum=0.99, epsilon=1e-3, center=True, scale=True)(x)
x = Bidirectional(CuDNNGRU(128, return_sequences=True), merge_mode='sum')(x)
x = Bidirectional(CuDNNGRU(128, return_sequences=True), merge_mode='sum')(x)
x = Bidirectional(CuDNNGRU(128, return_sequences=False), merge_mode='sum')(x)
x = BatchNormalization(axis=-1, momentum=0.99, epsilon=1e-3, center=True, scale=True)(x)
#Flatten layer
x = Flatten()(x)
#Dense Layer 1
x = Dense(256, activation='relu')(x)

outputs = Dense(1, activation="softmax")(x)
model = Model(inputs, outputs)
model.summary()

model.compile(loss='categorical_crossentropy', optimizer='nadam', metrics=['accuracy'])

early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=15, min_delta=0.0001) 
checkpoint = ModelCheckpoint('speech2text_model.hdf5', monitor='val_acc', verbose=1, save_best_only=True, mode='max')

print(x_train.shape)
print(y_train.shape)
print(x_valid.shape)
print(y_valid.shape)

hist = model.fit(
    x=x_train, 
    y=y_train,
    epochs=100, 
    callbacks=[early_stop, checkpoint], 
    batch_size=32, 
    validation_data=(x_valid,y_valid)
)

from keras import models  
#model = models.load_model("speech2text_model.hdf5")
def s2t_predict(audio, shape_num=10000):
    prob = model.predict(audio.reshape(1,shape_num,1))
    index = np.argmax(prob[0])
    return y_train[index]

import random
import IPython.display as ipd
index = random.randint(0,len(x_valid) - 1)
samples = x_valid[index].ravel()
#print("Audio:", classes[np.argmax(y_valid[index])])
#print("Predicted Text:", s2t_predict(samples))
ipd.Audio(samples, rate=1000)

#samplerate = 16000
# the value below must be in seconds  
#duration = 1
#filename = 'recorded_audio.wav'
#print("start")
#mydata = sd.rec(int(samplerate * duration), samplerate=samplerate,
#    channels=1, blocking=True)
#print("end")
#sd.wait()
#sf.write(filename, mydata, samplerate)

print(audio.shape)
print(audio_rate)

audio, audio_rate = librosa.load('/content/drive/MyDrive/KunFYP/audio_dataset2/A1.wav', sr=16000)
audio_sample = librosa.resample(audio, audio_rate, 1000)
ipd.Audio(audio_sample,rate=1000)
#print("Predicted Text:", s2t_predict(audio_sample))
Data=s2t_predict(audio_sample)

counter=0
for i in y:
    if i==Data:
       print(labels[counter])      
    counter+=1

"""# **Naive Bayes Model**"""

pip install sklearn

import numpy as np, pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import make_pipeline
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, accuracy_score

sns.set() # use seaborn plotting style

# Load the dataset
df = pd.read_csv('/content/drive/MyDrive/KunFYP/intent_dataset2.csv')
df = shuffle(df)
df.reset_index(drop=True)
print(df.head(10))
# Get the text categories
text_categories = ['turn on light', 'turn off light', 'turn on fan', 'turn off fan', 'speed up fan', 'slow down fan', 'open door', 'close door', 'open window', 'close window']
# define the training and testing set
X_train, X_test, Y_train, Y_test = train_test_split(df['Sentence'],df['Intent'], test_size = 0.10, random_state = 42)

print("We have {} unique classes".format(len(text_categories)))
print("We have {} training samples".format(len(X_train)))
print("We have {} test samples".format(len(X_test)))

# Build the model
model = make_pipeline(TfidfVectorizer(), MultinomialNB())
count_vect = CountVectorizer()
X_train_counts = count_vect.fit_transform(X_train)
tfidf_transformer = TfidfTransformer()
X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)
# Train the model using the training data
clf = MultinomialNB().fit(X_train_tfidf, Y_train)
# Predict the categories of the test data

print(X_test)
print(df['Intent'][509])
pred = clf.predict(count_vect.transform(X_test))

correct = 0
print(len(Y_test))
print(type(Y_test))
count = 0
for x in Y_test:
  if x == pred[count]:
    correct+=1
  count+=1

Accuracy = correct/len(Y_test)
print(f"Accuracy on test data = {Accuracy * 100}%")

"""**Test on a new Value**"""

test_sentence = ["گھر کے دروازہ بند ہوجائیں","پنکا چالا دو"]

val = clf.predict(count_vect.transform([test_sentence[0]]))

print(val[0])